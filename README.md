# <p align=center> Patient-Level Anatomy Meets Scanning-Level Physics: Personalized Federated Low-Dose CT Denoising Empowered by Large Language Model

 <div align="center">
     
[![Paper](https://img.shields.io/badge/SCAN--PhysFed-CVPR-red.svg)](https://cvpr.thecvf.com/virtual/2025/poster/34378)

</div>

---

This repository is a PyTorch implementation of SCAN-PhysFed (accepted by CVPR 2025). This paper can be downloaded at [this link](https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Patient-Level_Anatomy_Meets_Scanning-Level_Physics_Personalized_Federated_Low-Dose_CT_Denoising_CVPR_2025_paper.html).

> **Abstract:** *Reducing radiation doses benefits patients, but the resultant low-dose computed tomography (LDCT) images often suffer from clinically unacceptable noise and artifacts. While deep learning (DL) has shown promise in LDCT reconstruction, it requires large-scale data collection from multiple clients, raising privacy concerns. Federated learning (FL) has been introduced to mitigate these privacy concerns; however, current methods are typically tailored to specific scanning protocols, which limits their generalizability and makes them less effective for unseen protocols. To address these issues, we propose SCAN-PhysFed, a novel SCanning- and ANatomy-level personalized Physics-Driven Federated learning paradigm for LDCT reconstruction. Since the noise distribution in LDCT data is closely tied to scanning protocols and anatomical structures, we propose a dual-level physics-informed way to address these challenges. Specifically, we incorporate physical and anatomical prompts into our physics-informed hypernetworks to capture scanning- and anatomy-specific information, enabling dual-level physics-driven personalization of imaging features. These prompts are derived from the scanning protocol and the radiology report generated by a medical large language model (MLLM). Subsequently, client-specific decoders project these dual-level personalized imaging features back into the image domain. Besides, to tackle the challenge of unseen data, we introduce a novel protocol vector-quantization strategy (PVQS), which ensures consistent performance across new clients by quantifying unseen scanning codes to the closest match in the scanning codebook. Extensive experimental results demonstrate the superior performance of SCAN-PhysFed on public datasets.*

#### Citation
If our work is valuable to you, please cite our work:

```
@InProceedings{Yang_2025_CVPR,
    author    = {Yang, Ziyuan and Chen, Yingyu and Wang, Zhiwen and Shan, Hongming and Chen, Yang and Zhang, Yi},
    title     = {Patient-Level Anatomy Meets Scanning-Level Physics: Personalized Federated Low-Dose CT Denoising Empowered by Large Language Model},
    booktitle = {Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR)},
    month     = {June},
    year      = {2025},
    pages     = {5154-5163}
}
```


#### Requirements

If you've tried our previous work [HyperFed](https://github.com/Zi-YuanYang/HyperFed), please skip this step. 

Our codes were implemented by ```PyTorch 1.10``` and ```11.3``` CUDA version. If you wanna try our method, please first install necessary packages as follows:

```
pip install requirements.txt
```

#### Data Preparation

Our data simulation is based on [CTLib](https://github.com/xiawj-hub/CTLIB) in simulating data. If you have an interest in data simulation, we recommend installing it. Furthermore, HyperFed can be easily integrated into transformer-based methods with minor modifications.

Due to data copyright and privacy related issues, we are still working on obtaining permission to release the simulation data directly. However, you can use CTLib to perform the simulation based on the reported scanning protocols from our paper.

To ensure faster training, we will first extract the anatomy features for all images using [MiniGPT-Med](https://github.com/Vision-CAIR/MiniGPT-Med), and then store them. Please follow this step, as it will save a significant amount of training time.

#### Contact
If you have any question or suggestion to our work, please feel free to contact me. My email is cziyuanyang@gmail.com.

